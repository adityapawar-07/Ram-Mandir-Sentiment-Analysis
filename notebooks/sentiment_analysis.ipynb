{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTMt6vqrY0rk",
        "outputId": "d00d5496-499d-4cb4-9710-9f45b9b12eb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (2.155.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.19.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.66.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.25.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-api-python-client pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKvrj8I_WhTR",
        "outputId": "bca2ef48-9fa8-4825-caf9-58d8fa037871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 15536 comments to youtube_comments.csv\n"
          ]
        }
      ],
      "source": [
        "from googleapiclient.discovery import build\n",
        "import pandas as pd\n",
        "\n",
        "# Replace with your YouTube Data API key\n",
        "API_KEY = \"AIzaSyDaFmq3F7SBFQvL44zwSmPCSBAVxkwZuUA\"\n",
        "\n",
        "# Function to fetch comments from a video\n",
        "def fetch_comments(video_id):\n",
        "    youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
        "    comments = []\n",
        "    request = youtube.commentThreads().list(\n",
        "        part=\"snippet\",\n",
        "        videoId=video_id,\n",
        "        maxResults=100  # You can change this as needed\n",
        "    )\n",
        "\n",
        "    while request:\n",
        "        response = request.execute()\n",
        "        for item in response[\"items\"]:\n",
        "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
        "            comments.append(comment)\n",
        "\n",
        "        # Check for next page\n",
        "        if \"nextPageToken\" in response:\n",
        "            request = youtube.commentThreads().list(\n",
        "                part=\"snippet\",\n",
        "                videoId=video_id,\n",
        "                maxResults=100,\n",
        "                pageToken=response[\"nextPageToken\"],\n",
        "            )\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return comments\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Replace with your YouTube video ID\n",
        "    video_id = \"nWc3c1Lvp1Q\"\n",
        "    comments = fetch_comments(video_id)\n",
        "\n",
        "    # Save to a CSV file\n",
        "    df = pd.DataFrame(comments, columns=[\"Comment\"])\n",
        "    df.to_csv(\"youtube_comments.csv\", index=False, encoding=\"utf-8\")\n",
        "    print(f\"Saved {len(comments)} comments to youtube_comments.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "C9oVnUCAYoif",
        "outputId": "afc28b52-77f3-4f46-b9c2-7385204c911a"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_23523588-cd14-4091-8f7a-192248fb1db8\", \"youtube_comments.csv\", 2053335)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('youtube_comments.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qgfljN5qJn9",
        "outputId": "27fb5ab4-3d3c-49e1-b0f0-93e38c785078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data cleaned and saved as cleaned_youtube_comments.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"youtube_comments.csv\")\n",
        "\n",
        "# Cleaning function\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove special characters\n",
        "    text = text.lower().strip()  # Convert to lowercase and strip whitespace\n",
        "    return text\n",
        "\n",
        "# Apply cleaning\n",
        "df[\"cleaned_comments\"] = df[\"Comment\"].apply(clean_text)\n",
        "\n",
        "# Save cleaned data\n",
        "df.to_csv(\"cleaned_youtube_comments.csv\", index=False)\n",
        "print(\"Data cleaned and saved as cleaned_youtube_comments.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcBhW6FitpgD",
        "outputId": "c224ee67-c883-4e4a-d29b-43375be7349f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of cleaned comments: 15536\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of cleaned comments: {len(df['cleaned_comments'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54ieQXT9tRV_",
        "outputId": "d7ee342a-2e24-4833-dd51-d932feadedee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preparation complete. Saved as tfidf_vectorized_comments.csv.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "df = pd.read_csv(\"cleaned_youtube_comments.csv\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    tokens = text.split()\n",
        "    processed_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "    return \" \".join(processed_tokens)\n",
        "\n",
        "df['cleaned_comments'] = df['cleaned_comments'].fillna('')  # Handle NaN values\n",
        "df[\"processed_comments\"] = df[\"cleaned_comments\"].apply(preprocess_text)\n",
        "\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Use top 1000 features\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df[\"processed_comments\"])\n",
        "\n",
        "# Convert TF-IDF matrix to a DataFrame for easy viewing\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Save the TF-IDF DataFrame\n",
        "tfidf_df.to_csv(\"tfidf_vectorized_comments.csv\", index=False)\n",
        "print(\"Data preparation complete. Saved as tfidf_vectorized_comments.csv.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "iZ0g2yQZVhe8",
        "outputId": "68eb8c8d-9b8c-4d53-bdba-380fd61c6798"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_9a103718-4aab-4f0d-b25c-4e62acbc19c0\", \"tfidf_vectorized_comments.csv\", 63825326)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('tfidf_vectorized_comments.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozCiG8r6uKIf",
        "outputId": "baf8cfb7-6c20-45e5-adb5-8c4cefdae8ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             Comment  \\\n",
            "0  For daily updates follow us on Instagram: niti...   \n",
            "1           Tell us about Balbeer SingüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüòÖüòÖüòÖ   \n",
            "2                jai shree ram ji<br>jai bajrangbali   \n",
            "3                                   Jai Shree Ram ‚ù§‚ù§   \n",
            "4               Jai shree Ram ji <br>Jai bajrangbali   \n",
            "\n",
            "                                    cleaned_comments sentiment  \n",
            "0  for daily updates follow us on instagram nitis...   neutral  \n",
            "1                         tell us about balbeer sing   neutral  \n",
            "2                  jai shree ram jibrjai bajrangbali   neutral  \n",
            "3                                      jai shree ram   neutral  \n",
            "4                 jai shree ram ji brjai bajrangbali   neutral  \n",
            "Data labeled successfully and saved as 'labeled_youtube_comments.csv'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your YouTube comments (after cleaning)\n",
        "df = pd.read_csv(\"cleaned_youtube_comments.csv\")\n",
        "\n",
        "# Initialize a list to store labels\n",
        "labels = []\n",
        "\n",
        "# Function to manually label the data\n",
        "def label_data(comment):\n",
        "    # Ensure the comment is a string\n",
        "    if not isinstance(comment, str):\n",
        "        return \"neutral\"  # In case comment is NaN or not a string\n",
        "\n",
        "    # Here you can define conditions based on keywords or sentiment rules\n",
        "    if \"good\" in comment or \"love\" in comment:\n",
        "        return \"positive\"\n",
        "    elif \"bad\" in comment or \"hate\" in comment:\n",
        "        return \"negative\"\n",
        "    else:\n",
        "        return \"neutral\"\n",
        "\n",
        "# Apply the label_data function to each comment\n",
        "df[\"sentiment\"] = df[\"cleaned_comments\"].apply(label_data)\n",
        "\n",
        "# Optionally, print a few rows to check\n",
        "print(df.head())\n",
        "\n",
        "# Save the labeled data\n",
        "df.to_csv(\"labeled_youtube_comments.csv\", index=False)\n",
        "print(\"Data labeled successfully and saved as 'labeled_youtube_comments.csv'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqNav9WyyA9w",
        "outputId": "4cc7da94-d927-472b-9879-95aab842aa42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Counts:\n",
            "sentiment\n",
            "neutral     14490\n",
            "negative      527\n",
            "positive      519\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Positive Comments: 519\n",
            "Negative Comments: 527\n",
            "Neutral Comments: 14490\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the labeled data (with sentiment labels)\n",
        "df = pd.read_csv(\"labeled_youtube_comments.csv\")\n",
        "\n",
        "# Count the number of comments in each sentiment category\n",
        "sentiment_counts = df[\"sentiment\"].value_counts()\n",
        "\n",
        "# Print the counts for each category\n",
        "print(\"Sentiment Counts:\")\n",
        "print(sentiment_counts)\n",
        "\n",
        "# Optionally, you can also access the individual counts like this:\n",
        "positive_count = sentiment_counts.get(\"positive\", 0)\n",
        "negative_count = sentiment_counts.get(\"negative\", 0)\n",
        "neutral_count = sentiment_counts.get(\"neutral\", 0)\n",
        "\n",
        "print(f\"\\nPositive Comments: {positive_count}\")\n",
        "print(f\"Negative Comments: {negative_count}\")\n",
        "print(f\"Neutral Comments: {neutral_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwlXsGpIvyv2",
        "outputId": "d7b66366-1e46-435f-b733-f21057156a85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentiment\n",
            "neutral     14490\n",
            "negative      527\n",
            "positive      519\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "sentiment_counts = df[\"sentiment\"].value_counts()\n",
        "print(sentiment_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abBaSvkyvwHE",
        "outputId": "e93b6162-8eeb-400c-f30e-a5b43b1cab1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.0)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install imbalanced-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsoDQdYAv470",
        "outputId": "ea68e415-e241-4730-ade0-b5e61bb055e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before balancing:\n",
            "sentiment\n",
            "neutral     11571\n",
            "positive      429\n",
            "negative      428\n",
            "Name: count, dtype: int64\n",
            "\n",
            "After balancing:\n",
            "sentiment\n",
            "positive    11571\n",
            "neutral     11571\n",
            "negative    11571\n",
            "Name: count, dtype: int64\n",
            "Balanced dataset saved as 'balanced_dataset.csv'.\n"
          ]
        }
      ],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Load your dataset into a DataFrame\n",
        "# Replace 'your_dataset.csv' with the path to your dataset file\n",
        "df = pd.read_csv(\"labeled_youtube_comments.csv\")\n",
        "\n",
        "# Step 2: Handle missing values in the 'cleaned_comments' column\n",
        "df[\"cleaned_comments\"] = df[\"cleaned_comments\"].fillna(\"\")  # Fill NaN with empty string\n",
        "\n",
        "# Features and labels\n",
        "X = df[\"cleaned_comments\"]\n",
        "y = df[\"sentiment\"]\n",
        "\n",
        "# Vectorize the text data\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_vectorized = tfidf_vectorizer.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply SMOTE to the training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"Before balancing:\")\n",
        "print(y_train.value_counts())\n",
        "\n",
        "print(\"\\nAfter balancing:\")\n",
        "print(pd.Series(y_train_balanced).value_counts())\n",
        "\n",
        "# Convert the sparse matrix X_train_balanced back to a dense array for exporting\n",
        "X_train_balanced_dense = X_train_balanced.toarray()\n",
        "\n",
        "# Create a DataFrame from the resampled data\n",
        "balanced_df = pd.DataFrame(\n",
        "    X_train_balanced_dense,\n",
        "    columns=tfidf_vectorizer.get_feature_names_out()\n",
        ")\n",
        "balanced_df[\"sentiment\"] = y_train_balanced  # Add the resampled labels\n",
        "\n",
        "# Save the balanced dataset to a CSV file\n",
        "balanced_df.to_csv(\"balanced_dataset.csv\", index=False)\n",
        "print(\"Balanced dataset saved as 'balanced_dataset.csv'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "WUw7zb_G5mSI",
        "outputId": "19aacec9-34d3-4f1c-ee88-08315fd6f7ff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c46b11e6-5a9d-4956-9f25-7c8019f41f8c\", \"balanced_dataset.csv\", 713771715)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('balanced_dataset.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "df = pd.read_csv(\"balanced_dataset.csv\")\n",
        "\n",
        "# Separate features and labels\n",
        "X = df.drop(columns=[\"sentiment\"])  # Drop the target column\n",
        "y = df[\"sentiment\"]\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Train Models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"SVM\": SVC(kernel=\"linear\"),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"Training {model_name}...\")\n",
        "    model.fit(X_train, y_train)  # Train the model\n",
        "    y_pred = model.predict(X_test)  # Predict on test set\n",
        "    accuracy = accuracy_score(y_test, y_pred)  # Calculate accuracy\n",
        "    results[model_name] = accuracy\n",
        "    print(f\"{model_name} Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Step 3: Compare Models\n",
        "print(\"\\nModel Comparison:\")\n",
        "for model_name, accuracy in results.items():\n",
        "    print(f\"{model_name}: {accuracy:.2f}\")\n",
        "\n",
        "# Identify the best model\n",
        "best_model = max(results, key=results.get)\n",
        "print(f\"\\nBest Model: {best_model} with Accuracy: {results[best_model]:.2f}\")\n"
      ],
      "metadata": {
        "id": "Q4vLNT3pSzwG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f493a34-2114-4ffe-f1f2-f2f4ee6a64f6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Logistic Regression...\n",
            "Logistic Regression Accuracy: 0.96\n",
            "Training SVM...\n",
            "SVM Accuracy: 0.97\n",
            "Training Random Forest...\n",
            "Random Forest Accuracy: 0.99\n",
            "Training Naive Bayes...\n",
            "Naive Bayes Accuracy: 0.79\n",
            "\n",
            "Model Comparison:\n",
            "Logistic Regression: 0.96\n",
            "SVM: 0.97\n",
            "Random Forest: 0.99\n",
            "Naive Bayes: 0.79\n",
            "\n",
            "Best Model: Random Forest with Accuracy: 0.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the best model\n",
        "best_model_filename = f\"Best_Model_{best_model.replace(' ', '_')}.pkl\"\n",
        "joblib.dump(models[best_model], best_model_filename)  # Save the best model\n",
        "print(f\"Best model ({best_model}) saved as {best_model_filename}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnyFuChT3IwI",
        "outputId": "a5114efa-d12a-4a25-9eaa-dda8bbd0991a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model (Random Forest) saved as Best_Model_Random_Forest.pkl.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the saved model\n",
        "best_model_filename = \"Best_Model_Random_Forest.pkl\"  # Replace with your actual file name\n",
        "model = joblib.load(best_model_filename)\n",
        "\n",
        "# Load the vectorizer (recreate or use a saved instance if available)\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "tfidf_vectorizer.fit_transform(pd.read_csv(\"balanced_dataset.csv\").drop(columns=[\"sentiment\"]))\n",
        "\n",
        "# Preprocessing function\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove special characters\n",
        "    text = text.lower().strip()  # Lowercase and strip whitespace\n",
        "    tokens = text.split()\n",
        "    processed_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "    return \" \".join(processed_tokens)\n",
        "\n",
        "# Predict function\n",
        "def predict_sentiment(input_texts):\n",
        "    # Ensure input is a list\n",
        "    if isinstance(input_texts, str):\n",
        "        input_texts = [input_texts]\n",
        "\n",
        "    # Preprocess input texts\n",
        "    preprocessed_texts = [preprocess_text(text) for text in input_texts]\n",
        "\n",
        "    # Vectorize texts\n",
        "    input_vectorized = tfidf_vectorizer.transform(preprocessed_texts)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(input_vectorized)\n",
        "    return predictions\n",
        "\n",
        "# Example input\n",
        "input_texts = [\n",
        "    \"I love this video, it was amazing!\",\n",
        "    \"This content is terrible and I hate it.\",\n",
        "    \"The video was okay, nothing special.\"\n",
        "]\n",
        "\n",
        "# Predict sentiments\n",
        "predicted_sentiments = predict_sentiment(input_texts)\n",
        "for text, sentiment in zip(input_texts, predicted_sentiments):\n",
        "    print(f\"Comment: {text}\\nPredicted Sentiment: {sentiment}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Xspb042t7C7",
        "outputId": "c6e4fc1e-1238-4c7c-832b-c4782f2a634d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comment: I love this video, it was amazing!\n",
            "Predicted Sentiment: positive\n",
            "\n",
            "Comment: This content is terrible and I hate it.\n",
            "Predicted Sentiment: negative\n",
            "\n",
            "Comment: The video was okay, nothing special.\n",
            "Predicted Sentiment: neutral\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(tfidf_vectorizer, \"tfidf_vectorizer.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hE1764A0xyiq",
        "outputId": "a64a5952-2f1d-4957-d082-dc6ee61cc434"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tfidf_vectorizer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}